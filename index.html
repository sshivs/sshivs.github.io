<!DOCTYPE html>
<html class="gr__sshivs_github_io"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Shiv Shankar</title>

    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

  </head>
  <body data-gr-c-s-loaded="true">
    <div class="wrapper">
      <section>

<p>
  <font size="6">Shiv Shankar</font> <br>
        Ph.D. Candidate, Computer Science <br>
        <a href="http://www.iesl.cs.umass.edu/">Information Extraction and Synthesis Lab</a> <br>
        <a href="https://groups.cs.umass.edu/mlds/">UMass Machine Learning for Data Science Lab</a> <br>
        Email: sshankar at umass.edu <br>
        <a href="https://scholar.google.com/citations?user=yK56jugAAAAJ" target="_blank" style="padding-left: 0px"><img src="images/scholar.png" width="24px" height="24px"></a>
              

            </p>

<br>
<br>


<p align="justify">I am a Ph.D. candidate in Computer Science
 at University of Massachusetts Amherst. My primary research interest is in probabilistic machine learning, including topics in inference/structure prediction, quantum machine learning, generative models and deep learning. I am particularly motivated by applications related to sustainability, resource efficient learning and intepretability.
 </p>





<p>
Before starting my PhD, I was primarily a strat-quant working on financial math, derivative pricing and market making. During that stint I got the chance to
sharpen my, now admittedly rusty, dev skills on low latency programming. I obtained my bachelors in Computer Science and Engineering from IIT Bombay.
</p>







<h1> Publications</h1> 

<ol>

  <li><b> Differential Equation Units: Learning Functional Forms of Activation Functions from Data </b> 
    <br>
    Mohamadali Torkamani, <b>Shiv Shankar</b>, Amirmohammad Rooshenas, Phillip Wallis
<br>
AAAI Conference on Artificial Intelligence (AAAI), 2020.
<br>
[<a href="https://arxiv.org/pdf/1909.03069">paper</a>][<a href="https://github.com/rooshenas/deu">code</a>]
  </li>
  <br>

  <li> <b>Three-quarter Sibling Regression for Denoising Observational Data</b>
  <br>
  <b> Shiv Shankar</b>, Daniel Sheldon, Tao Sun, John Pickering, Thomas Dietterich
  <br>
International Joint Conference on Artificial Intelligence (IJCAI), 2019. 
<br>
  [<a href="https://people.cs.umass.edu/~sheldon/papers/3qs.pdf">paper</a>]
</li>
<br>
<li> <b> Posterior Attention Models for Sequence to Sequence Learning
</b>
<br>
<b>Shiv Shankar</b>, Sunita Sarawagi
<br>
International Conference on Learning Representation (ICLR), 2019.  
<br>
[<a href="https://arxiv.org/abs/1803.10459">paper</a>]
<br>
</li>

<br>
<li> <b> Surprisingly Easy Hard Attention for Sequence to Sequence Models
</b>
<br>
<b>Shiv Shankar</b>, Siddhant Garg, Sunita Sarawagi
<br>
Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018.  
<br>
[<a href="https://www.aclweb.org/anthology/D18-1065.pdf">paper</a>][<a href="https://github.com/sid7954/beam-joint-attention">code</a>]
<br>
</li>

<br>
<li> <b> Generalizing across Domains via Cross-Gradient Training
</b>
<br>
<b>Shiv Shankar</b>*, Vihari Piratla*, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi, Sunita Sarawagi
<br>
International Conference on Learning Representation (ICLR), 2018.  
<br>
[<a href="https://arxiv.org/abs/1804.10745">paper</a>][<a href="https://github.com/vihari/crossgrad">code</a>]
<br>
</li>

<br>
<li> <b> Labeled Memory Networks for Online Model Adaptation
</b>
<br>
<b>Shiv Shankar</b>, Sunita Sarawagi
<br>
AAAI Conference on Artificial Intelligence (AAAI), 2018. 
<br>
[<a href="https://arxiv.org/abs/1707.01461">paper</a>][<a href="https://github.com/sshivs/LMN">code</a>]
<br>
</li>

[* denotes equal contribution]
<p></p>


    </div>

        
  

</body></html>
